<INTRODUCTION>
Attention Mechanism is a pivotal technique in deep learning, primarily used to address the issue of long-range dependencies in sequential data, like text or voice. This concept is modelled on human attention, where it is inherently understood that not all parts of input information are equally important. Therefore, the model learns to assign different weights of importance to different parts of the input when making predictions. Fundamentally, Attention Mechanism attempts to boost the performance of deep learning models by enabling them to focus on specific parts of the input sequentially.

<HISTORY>
The concept of Attention Mechanism in deep learning was introduced by Dzmitry Bahdanau et al. in their 2014 research paper. The initial objective was to improve the results of Neural Machine Translation (NMT). The primary issue with NMT was that it tried to encode the whole source sentence into a fixed-length vector from which to decode the target sentence, which made it difficult for the model to handle long sentences. The concept of Attention Mechanism was proposed to solve this particular issue - to selectively concentrate on a few relevant things, while ignoring the less relevant ones.

<KEY IDEAS>
Attention Mechanism offers a solution to manage and prioritize the sequence's elements. It allows models to assign different degrees of importance to various parts of the input. Notably, there are two main types: Global Attention where all the input parts are considered when assigning relevance, and Local Attention, where only some parts of the input sequence are given priority. The foundation of Attention Mechanism lies upon three core components: the Query, which is related to the current time-step; the Key, related to all possible time-steps; and the Value, which are the encoded input features. 

<VARIATIONS>
Over the years, numerous variations of Attention Mechanism have been developed to improve on the original idea and expand its usage. Some popular variants include additive attention, multiplicative (dot-product) attention, scale-dot product attention, and self-attention. Additionally, these attention mechanisms vary in computational effort, interpretability, and scaling with the dimensionality of queries and keys. Transformer models, introduced in 2017 by Vaswani et al., use self-attention mechanisms extensively and have become state-of-the-art models for many Natural Language Processing tasks.

<APPLICATIONS>
The applications of Attention Mechanism span across a wide range of machine learning tasks. It is significantly used in various Natural Language Processing tasks like machine translation, text summarization, sentiment analysis, among others. Moreover, it's employed in computer vision tasks to allow the model to "focus" on specific parts of an image while making predictions. Furthermore, it is also used in speech recognition systems and even reinforcement learning. Its successful application in Google's Transformer model has made it an essential technique for developing advanced Deep Learning models.