<INTRODUCTION>
Semantic Role Labeling (SRL) is a task in the field of Natural Language Processing (NLP) that involves determining the semantic roles of words or phrases in a sentence. This method assigns roles to phrase-level constituents, providing a description of abstract syntactic structures. The goal with SRL is to understand the intent of the author by categorizing the underlying roles and semantic relationships in sentences. This is an essential component in creating systems that can comprehend natural language and helps machine understand humans more accurately.

<HISTORY>
Semantic Role Labeling has its roots in the linguistic theory of semantic roles, which dates back to the 1960s. The basic idea is to identify and classify how various arguments of a sentence are related to the verb's meaning. The term “Semantic Role Labeling” was formally introduced by Daniel Gildea and Daniel Jurafsky in their 2002 paper, ushering in an era of SRL systems based on machine learning algorithms. From then, with the growth of deep learning models, there has been significant progress in developing more accurate SRL models. 

<KEY IDEAS>
The key ideas behind Semantic Role Labeling involve classifying phrases or words as 'arguments' based on their syntactic and semantic roles. For example, in a sentence, phrases can be classified as 'agent', 'patient', 'instrument' etc. based on their relationship with the verb. Modern SRL techniques often use machine learning models which are trained on corpora annotated with semantic roles. These models predict semantic roles based on the syntactic structure of the sentence and the contextual patterns in the corpora. SRL extends the concept of part-of-speech tagging and syntactic parsing, giving more importance to semantic understanding rather than just syntactic structure.

<VARIATIONS>
There are several variations in Semantic Role Labeling approaches, primarily focused on two dimensions: Predicate-argument structure and Grammatical relation approach. In the predicate-argument structure, the focus is mainly on verbs and their arguments, providing a more refined view of sentence semantics. On the other hand, the grammatical relations approach takes into account all words and relations, striving for a broader and more comprehensive view of sentence semantics. Deep learning has also enabled the development of end-to-end SRL systems, such as the Dependency-based SRL, which operate directly on a sentence without the need for external syntactic parsing.

<APPLICATIONS>
Semantic Role Labeling is integral to various practical applications in Natural Language Processing. It can be used in information extraction where it can help identify relations between entities, improving the details extracted from text. SRL also boosts tasks like machine translation, as understanding semantic roles can improve the translation of predicates across languages. It can also boost sentiment analysis by giving a better understanding of who did what to whom in a given text. Moreover, SRL plays a key role in question answering systems, helping the system decipher the semantic constituents of a user's question to generate accurate answers.