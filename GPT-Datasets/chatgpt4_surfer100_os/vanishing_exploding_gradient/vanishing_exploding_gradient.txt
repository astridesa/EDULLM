<INTRODUCTION>
The Vanishing & Exploding Gradient problem refers to a significant challenge faced during training Deep Neural Networks. At its core, it is a difficulty that arises due to the gradient descent optimization during backpropagation, with gradients either becoming excessively large (exploding) or exceedingly small (vanishing). It has considerable impact on the learning process of neural networks, often leading to unstable or slower training. The exploration of solutions to tackle the vanishing and exploding gradient problem is a pivotal area within the realm of deep learning.

<HISTORY>
The vanishing gradient problem was identified early in the research of neural networks, noted in the 1980s and 1990s. In 1991, Sepp Hochreiter noted it in his Diploma thesis, but the term was coined later by Bengio et al. in 1994. The exploding gradient problem was recognized as an underlying issue in Recurrent Neural Networks (RNN), first outlined by Pascanu et al. in 2013. Both problems are fundamental issues in deep learning, influencing the learning performance of deep neural networks.

<KEY IDEAS>
The crux of the vanishing gradient problem is that during backpropagation in deep networks the gradients can become exceedingly small. This results in the network weights not updating significantly, hence learning fails to progress. The exploding gradient problem, on the other hand, occurs when large error gradients lead to large updates to network weights during learning. An excessive update can lead the learning process to become unstable. The core concepts for mitigating these issues are varied, including weight initialization methods, gradient clipping, batch normalization or architectural innovations such as LSTM (Long Short-Term Memory) or GRU (Gated Recurrent Unit) networks for RNNs.

<VARIATIONS>
Different strategies to deal with vanishing and exploding gradients have evolved over time and include a combination of both novel techniques and innovative architectures. Weight normalization, gradient clipping, skip connections (as in ResNets), Long Short-Term Memory (LSTM), Gated Recurrent Units (GRU), and batch normalization are among the most commonly used methods. Variations of these techniques often involve tradeoffs between complexity, performance, and computational overhead, making the choice of method application-dependent.

<APPLICATIONS>
Addressing vanishing and exploding gradients is fundamental for the proper training and performance of deep neural networks, impacting applications in fields like computer vision, natural language processing, speech recognition, and many more. For example, LSTM and GRU architectures, known for their ability to mitigate these issues, are popular in natural language processing tasks such as machine translation and text generation. Solutions to these problems have also been critical in the training of deeper and thus more powerful networks in areas such as image and speech recognition.