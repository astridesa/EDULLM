<INTRODUCTION>
Dependency Parsing is a technique in syntax analysis that studies the grammatical structure of a sentence, based on the dependency between the words. It considers the words of a sentence as nodes and the grammatical relationships as edges. Each sentence constructs a dependency tree where nodes represent the words, and edges form between dependent words. The main principle of dependency parsing is that syntactic structure consists of relations between words, with one word often being central to understanding the sentence. Developed as a linguistics practice, it has found its role in Natural Language Processing (NLP), and plays an integral part in tasks like machine translation and information extraction.

<HISTORY>
The concept of dependency parsing has been around since ancient times, however, it wasn’t until the mid-20th century that it started to take form as a linguistic concept. Lucien Tesnière, a French linguist, first introduced the concept of dependency grammar in "Éléments de syntaxe structurale" in 1959. Tesnière’s dependency grammar model focused more on the semantic relationships between words rather than grammatical structures. Since then, various forms of dependency grammar and related parsing algorithms have been established and refined over the decades, particularly in sync with the rise of machine learning and computational linguistics.

<KEY IDEAS>
The primary principle of dependency parsing is considering a sentence as a series of binary asymmetric relations between words. In this model, one word (called the head) governs another word (a dependent). String-based models and graph-based models are the two key approaches for dependency parsing. The former emphasizes on the linear order of words in a sentence whereas the latter focuses on modeling entire sentences as structured graphs. The techniques often involve statistical learning and machine learning techniques to solve parsing problems. This combination of linguistic theory and computation has made dependency parsing a cornerstone of modern NLP.

<USES/APPLICATIONS>
Dependency parsing has significant applications in many areas of NLP. It's critical to information extraction tasks, including Named Entity Recognition and Relation Extraction where understanding the grammatical roles of words in a sentence is crucial. It plays a vital role in sentiment analysis, as it helps in identifying the subject and predicate of a sentence, assessing the sentiment polarity. It is also fundamental to machine translation applications, as it aids in understanding the original language syntax and semantics to produce a faithfully translated output.

<VARIATIONS>
Over time, many variations and advancements have emerged to improve the accuracy and complexity of dependency parsing. Transition-based parsing, graph-based parsing, and feature-based parsing are common variations. Each differs in the complexity and nature of their implementation. Transition-based parsing relies on local decisions, while graph-based parsing considers the entire sentence. Feature-based parsing helps in identifying accurate features using machine learning algorithms. Recently, neural network-based parsing models gained traction due to their robustness and efficiency in handling the complex relations in languages.