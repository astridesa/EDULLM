<INTRODUCTION>
Residual Neural Network or ResNet is an influential neural network structure that introduced the breakthrough concept of "skip connections". It revolutionized the field of Deep Learning as it addressed the problems of training extremely deep neural networks. ResNet enabled the training of networks with over 100 layers, efficiently handling the vanishing and exploding gradients problem. This network has been widely utilized in many areas, particularly in image and video processing, due to its ability to handle complex and composite patterns.

<HISTORY>
Residual Neural Networks were first introduced during the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2015 by a team of researchers from Microsoft. The primary authors, Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, introduced the concept of "residual learning" to tackle the limitations encountered in training deeper networks. Before ResNet, deep models experienced an issue of accuracy saturation and degradation, where drastic performance drop was observed with increasing depth. The introduction of ResNet successfully addressed this issue, winning the 2015 ILSVRC competition. 

<KEY IDEAS>
The fundamental innovation with Residual Networks lies in the introduction of 'skip connections' or 'shortcut connections'. These connections allow the model to skip layers during training, which helps to overcome the problem of vanishing gradients in very deep neural networks. In traditional network structures, each layer learns new features; in contrast, with ResNet, each layer learns the residual, or differences, between the features learned in the previous layer and the "ideal" one. This routine makes it possible to effectively train an exceptionally deep neural network without encountering issues associated with learning feature identifiers.

<VARIATIONS>
Several variations of Residual Networks have been proposed since its inception. Pre-activation ResNet, proposed by the original authors in the same year, is one such variant where the order of convolution and activation functions are altered to improve performance. DenseNet, another popular variant, extends the notion of skip connections by connecting each layer to every other layer in a feed-forward fashion, thus improving gradient flow. Meanwhile, Wide ResNet increases the width rather than depth to improve the modelâ€™s performance.

<APPLICATIONS>
ResNet has found widespread applications in many areas of computer vision, due to its capability to efficiently learn from high-dimensional and high-volume data. This includes tasks such as object detection, facial recognition, and even in medical applications for disease detection in X-ray images. It is also used in natural language processing for the classification, named entity recognition, and machine translation tasks, replacing traditional RNNs due to its effective handling of long sequences. ResNet architecture is integral to many real-world applications and research areas, impacting sectors from self-driving technologies to medical imaging automation.