<INTRODUCTION>
Gradient Boosted Decision Trees (GBDT) is a powerful machine learning technique that utilizes the concept of boosting to generate an ensemble of decision trees by optimizing a differentiable loss function. Predominantly used for regression and classification problems, GBDT can handle both categorical and numerical features and also takes care of missing values. It works by learning from the mistakes of previous trees and incrementally improving the model's predictions. Throughout its existence, GBDT has remained an important component in the realm of machine learning providing excellent accuracy rates and has found applications in various fields. 

<HISTORY>
The technique was initially developed in the context of decision tree algorithms and later applied to statistical learning frameworks. It was first introduced by Leo Breiman in 1997 and it was built upon the idea of boosting weak learners in an iterative fashion. Since then, it became a top-performing algorithm, continuously refined by machine learning researchers across the globe. Friedman's Gradient Boosting Machine (GBM) in 2001 introduced a general form of boosting method which proved to be effective and massively improved its popularity among data scientists due to its capabilities in solving various prediction problems.

<KEY IDEAS>
The foundation of GBDT is based on building an ensemble of weak prediction models, typically decision trees. The key idea is the use of a loss function and the method of gradient descent to minimize errors. After a tree is built, it calculates the difference between the actual and predicted values of the target variable, known as residuals. A new tree is then generated that learns these residuals instead of the actual target variable. This process is iteratively repeated until the algorithm reaches a specified number of trees or a minimum error threshold. The final prediction is the sum of the predictions from all the trees. This combination of weak models results in the creation of a strong, robust model.

<VARIATIONS>
Several variations of GBDT have been developed over the years, such as XGBoost, LightGBM, and CatBoost, which significantly improve speed and accuracy. XGBoost, or Extreme Gradient Boosting, is a high-performing and computationally efficient model that introduces the concept of regularization to control model complexity and avoid overfitting. LightGBM, developed by Microsoft, uses a novel technique of Gradient-based One-Side Sampling (GOSS) to filter out the data instances for finding the split value. CatBoost from Yandex technology provides support for categorical variables and implements symmetric trees and oblivious trees.

<APPLICATIONS>
GBDT, with its various versions, has a wide range of applications. It is widely used in Kaggle competitions and machine learning projects due to its accuracy and efficiency. Applications include but are not limited to price prediction, customer retention, fraud detection, and diagnosis systems. GBDT is also used in the field of bioinformatics for predicting gene classifications. Quite recently GBDT has found increasing applications in natural language processing and recommendation systems.