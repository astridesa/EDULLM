<INTRODUCTION>
A Document Term Matrix (DTM) is a mathematical matrix that represents information about words in a document. In this matrix, documents/rows correspond to the various documents within a corpus and the columns/terms correspond to different terms within the corpus. The value in each cell is generally an indication of the frequency that the term appears in its corresponding document. The DTM forms the backbone of many text analysis processes and has major implications in the field of machine learning and information retrieval.

<HISTORY>
The concept of Document Term Matrix traces its roots back to the inception of Information Retrieval (IR) systems. Early IR systems, during the mid 20th century, relied on boolean representations of documents where terms were simply marked present or absent. However, the introduction of statistical models in IR set the stage for the development of DTM. Its popularization occurred with the advent of the Vector Space Model in the 1960s by Salton et al., which used DTM to represent and retrieve information.

<KEY IDEAS>
The key idea behind the Document Term Matrix is the representation of documents as vectors in a high dimensional space, where each dimension corresponds to a unique term from the document corpus. This conversion facilitates the application of mathematical concepts for document comparison, categorization, and clustering. DTM is built on the notion of 'Bag of Words', where word order in the document is disregarded and only their frequency counts are considered. It is through this representation that the principle of term frequency-inverse document frequency (TF-IDF), used to assign importance to terms in a document, was derived.

<VARIATIONS>
There are variations in the way a Document Term Matrix can be constructed. One of the most common variations is the binary representation, where the presence of a term in a document is marked as 1, and absence as 0, disregarding frequency. Another variation is the TF-IDF weighted matrix, which weighs the term frequencies by their inverse document frequencies, to highlight the terms that are more unique to specific documents. Other variations include the use of normalization techniques to adjust for variations in document length and cutting-edge methods like Latent Semantic Indexing (LSI) which model associations between terms that are not explicitly apparent.

<APPLICATIONS>
The applications of the Document Term Matrix are broad and multifaceted. It is extensively used in information retrieval systems for document indexing, similarity comparison, and relevance scoring. In machine learning, DTM forms the basis for many natural language processing tasks like text classification, sentiment analysis, document clustering, and topic modeling. Also, in the realm of search engine technology, DTM plays a critical role in understanding user queries and identifying the most relevant documents from large corpora. With the advent of Big Data, its usage has extended to fields like bioinformatics, digital humanities, social media analytics, and more.