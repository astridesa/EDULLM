<INTRODUCTION>
Text generation, a subfield of Natural Language Processing (NLP), involves the automatic creation of written text. It utilizes various algorithms to generate human-like text based on certain input criteria. Generally powered by machine learning, text generation can imbibe a style or format and then create text that aligns with it. It's critical in countless applications, such as chatbots, content creation tools, or even AI writers. By mimicking the mechanisms through which humans write content, these algorithms offer machines the ability to write like humans do.

<HISTORY>
Early text generation was based on rule-based systems, using pre-defined templates filled with data. With the advent of machine learning, methods evolved to take context and semantics into account. Early machine learning methods include N-grams and Hidden Markov Models. The breakthrough of modern text generation was the development of Recurrent Neural Networks (RNNs), especially their Long Short-Term Memory (LSTM) variant. LSTM was remarkable for preserving contextual information over long text sequences. The recent wave of transformer-based models, such as GPT-3 and BERT, have advanced text generation further, enabling more sophisticated generation tasks.

<KEY IDEAS>
There are several key ideas in text generation. Word embeddings or vector representations of words are crucial as they help in understanding words in relation to each other. Techniques like Word2Vec and GloVe facilitate this. Recurrent Neural Networks, capable of learning patterns in time-series data, play a role in text generation, as do LSTMs, capable of retaining information over long sequences. Attention mechanism, introduced with Transformer models, aids in focusing on relevant parts of the input sequence, improving the quality of generated text. Lastly, the concept of fine-tuning the pre-trained models on specific tasks to achieve better performances is fundamental in text generation.

<VARIATIONS>
There are several variations to text generation, aligned to the range of tasks it can solve. These include Data-to-Text, where structured data is converted into natural language; Text-to-Text, where a piece of text is transformed into another text; and Language Modelling, which predicts the next word or sequence in a sentence. Each variant requires different approaches and models. For instance, transformers are often used for text-to-text tasks, while language modelling make use of recurrent neural networks.

<APPLICATIONS>
Text generation finds applications in various fields. In conversation AI, it is used to generate human-like responses; in newsrooms, for drafting simple reports; and in e-commerce, for creating product descriptions. It also plays a role in automatic email generation, content marketing, and social media posts. More advanced applications include writing film scripts, poem writing and novel writing, while in academic settings, it has been used in creating research papers and reports. Essentially, any scenario that requires a substantial amount of writing can leverage text generation to improve efficiency and scalability.