**Introduction**  
Text Generation involves the use of algorithms and models to produce textual content that is relevant, coherent, and grammatically correct. The base of such automated systems is established on techniques from Natural Language Processing (NLP), machine learning, and linguistics. These methods help machines interpret and generate human language in a way that's comprehensive and contextually accurate. The focus of text generation includes but is not limited to generating captions, composing poetry, writing narratives, and creating news articles.

**History**
The concept of text generation has historical roots in the mid-20th century, with the advent of basic language software. However, it truly took off with the growth of machine learning and AI in the 21st century. Early efforts at text generation often resulted in nonsensical syntax and limited relevance. However, later advances including the development of Hidden Markov Models (HMM), Recurrent Neural Networks (RNN), and more recent transformer-based models like GPT-3 and BERT have achieved remarkable success in producing human-like text.

**Key Ideas**
The core of Text Generation is anchored on understanding and mimicking human language intricacies. Essential techniques include using statistical language models to predict word occurrence probabilities, using machine learning to understand contextual semantics, and leveraging transformer models for sequence generation. Recent strides incorporate techniques such as attention mechanisms and transformer architectures. Technologies sometimes also utilise reinforcement learning strategies to improve the quality of generated text over successive iterations.

**Variations**
The field of text generation has a host of variants. Depending on requirement, these encompass simple models used for next-word prediction, to more advanced forms dealing with speech recognition, and now transformer-based models for complex tasks. A significant recent variation involves Conditional Text Generation. This is where a model produces text based on given conditions or context. Another variant, Neural Storytelling, generates creative and relatively lengthy pieces of text such as stories or essays.

**Applications**
Text generation has a myriad of applications that enhance various areas of life. For example, in journalism, it aids in rapid news generation; in entertainment, it helps create customised video game dialogues. In business, it is often exploited for automatic report generation and content creation for marketing purposes. Additionally, text generation also plays a paramount role in user interface design, where it helps machines communicate with humans more naturally and efficiently. The expanding potential for text generation applications paints a promising future for this domain of AI.