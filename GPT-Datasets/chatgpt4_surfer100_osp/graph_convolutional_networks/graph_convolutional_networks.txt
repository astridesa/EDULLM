<INTRODUCTION>
Graph Convolutional Networks (GCNs) are a robust computational model that uses the principles of convolutional neural networks and implements them on graph data. This machine learning model is a subfield of Graph Neural Networks and it gained popularity primarily in the fields of social network analysis, biomolecule properties, and computer vision. GCNs have been effectively used to delve into the geometric structure of data, thus relying on the inherent nature of graph theory. The motivation behind the concept of GCNs is to improve representation learning by exploiting the properties of graphs to capture richer patterns and structures in data.

<HISTORY>
The idea of GCNs was proposed by Thomas N. Kipf and Max Welling in their 2016 work in the context of semi-supervised learning. The model was introduced to address the limitations of feed-forward neural networks and recurrent neural networks, with their inability to handle complex structures like graphs. The key challenge that GCNs address is the representation learning problem for graph-structured data, which weren't well managed by traditional convolutional neural networks. This method successfully brought the power of deep learning algorithms to graph data, marking a significant advancement in the field. 

<KEY IDEAS>
GCNs exploit the local connection patterns in graphs for semi-supervised learning. The primary distinction between traditional convolutional networks and GCNs is that the latter involves an extension to irregular, non-Euclidean data. These networks function by focusing on the graph's adjacency matrix and combining it with the degree matrix to propagate signals through layers of the network. By stacking multiple graph convolution layers, a GCN can extract high-level features from node neighborhoods of increasing sizes. The convolution operation involves taking a hidden state from a node and its neighbors and then passing it through a shared filter or weights matrix. 

<USES/APPLICATIONS>
GCNs have a wide range of applications primarily due to their ability to handle complex structures. In social network analysis, GCNS have been employed to predict the behaviors, preferences, or affiliations of users. In the biomolecular field, GCNs are used to predict the properties of molecules and compounds. Significant strides have also been made in the realm of computer vision, with 3D object recognition applications. Additionally, GCNs have been effective in traffic forecasting, financial fraud detection, recommendation systems, and other scenarios that call for exploiting relationships and dependencies among data points.

<VARIATIONS>
Several variations of the Graph Convolutional Networks have been developed to expand the scope and efficacy of the method. Graph Attention Networks (GATs) allow weights to differ based on the relevance of neighboring nodes. Graph Isomorphism Networks (GINs) capture richer graph structures by adding more expressiveness to the model. GraphSAGE allows the inclusion of feature information of nodes capturing further graph details. Spectral GCN, ChebyNet are other variations that focus on spectral approaches. Moreover, the development of GCNs introduced a novel path in the broader field of Graph Neural Networks, leading to significant advancements in graph-based representation learning.