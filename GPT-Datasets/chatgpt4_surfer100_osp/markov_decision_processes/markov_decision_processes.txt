<INTRODUCTION>
Markov Decision Processes (MDPs) are a mathematical framework used for modeling decision-making situations where outcomes are partly random and partly under an agent's control. Predominantly used in computer science and operations research, MDPs are central to reinforcement learning. They provide a method to model and solve problems involving sequential decision-making under uncertainty, such as automated controls, games, or optimization problems. The main idea is to abstract a problem into states, actions, rewards, and transitions, which maps decision possibilities into future scenarios.

<HISTORY>
The concept of MDPs was introduced in the 1950s by mathematician Richard Bellman, who was studying optimization problems in dynamic systems. He introduced the "Principle of Optimality" which forms the basis of MDPs. The method has roots in statistics, specifically the Markov Chains of Andrey Markov. Initially, MDPs were predominantly used for solving operational research problems, but their use was expanded to other areas, including computer science, when the foundational aspects of reinforcement learning were established.

<KEY IDEAS>
The fundamental concept of MDP is the "Markov property", which states that the future state depends only on the current state and action, not on the sequence of events that preceded it. MDPs consist of four elements: a set of states, a set of actions, a reward function, and a transition probability matrix. An agent interacts with the process by choosing actions, the process moves to a new state, and the agent receives a reward. The goal of the agent is to maximize its cumulative reward over a sequence of steps. MDPs are solved by finding the optimal policy, which is a function from states to actions that maximizes the expected reward.

<USES/APPLICATIONS>
Markov Decision Processes are extensively used in a variety of fields including economics, medicine, robotics, artificial intelligence, and operations research. In computer science, MDPs are often used in reinforcement learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Other applications include process control (like industrial processes or communications networks), decision-making in healthcare (like treatment decisions or resource allocation), and in robotics for optimal path planning and decision-making under uncertainty.

<VARIATIONS>
Several variations of the standard MDP model exist to cater to a variety of problem scenarios. Partially Observable MDPs (POMDPs) extend MDPs to situations where the state of the environment is only partially observable. Semi-Markov Decision Processes (SMDPs) replace the Markov property with a broader dependency on the past. Continuous and Multi-objective MDPs handle continuous state spaces and multiple competing objectives respectively. These variants and others fit into the broader field of stochastic optimization and control, demonstrating the versatility and adaptability of the MDP framework.