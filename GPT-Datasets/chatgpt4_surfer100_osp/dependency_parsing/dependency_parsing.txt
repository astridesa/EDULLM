<INTRODUCTION>
Dependency Parsing is a fundamental task in natural language processing (NLP) that involves the study of grammatical dependencies within sentences. As a subfield of syntax in computational linguistics, it focuses on demonstrating the relationships between words based on their dependency. This technique is highly applicable for machine translation, information extraction, and syntax-based reordering for statistical machine translation, among others. It seeks to understand the structural relationship among words to enable a more comprehensive comprehension of language use.

<HISTORY>
The concept of Dependency Parsing was first embodied in the developments of computational linguistics during the 1950s. This technique was intended to be a solution to the intricate interaction of grammar and semantics in understanding sentences. The term "dependency" was introduced in Tesni√®re's work "Structural Syntax" in 1959, which provided a fresh perspective on sentence structure that focuses on the relationships between words rather than the traditional hierarchical structure.

<KEY IDEAS>
Dependency Parsing is anchored on the principle that syntactic structure consists of binary asymmetrical relations. It posits that every sentence is a linked structure composed of nodes representing words. Certain words (heads) connect to other words (dependents), creating a tree-structured graph called a Dependency Tree. The root of the tree signifies the main verb, while arrows pointing from heads to dependents denote dependency relations. The effectiveness of this approach stems from its ability to reveal the grammatical structure and semantic relationships within sentences.

<USES/APPLICATIONS>
Dependency Parsing forms the skeleton of a wide array of NLP tasks. It facilitates a clear understanding of text semantics, making it particularly useful in machine translation - converting a sentence in one language to another. It also plays an indispensable role in question-answering systems, text summarization, and information extraction, as it unearths the relationships among words. Moreover, dependency parsing aids in opinion mining and sentiment analysis by providing a detailed morphology of sentences.

<VARIATIONS>
There exist numerous models of Dependency Parsing, each varying in complexity and efficiency. Some of these variations include the Transition-based models which parse sentences from left-to-right in linear time, and the Graph-based models known for their global optimization. Another notable variant is the Constraint-based models, which base their parsing decisions on a set of linguistic constraints. The diverse nature of Dependency Parsing models fosters flexibility, allowing for a rich exploration of language semantics within the broader field of computational linguistics.