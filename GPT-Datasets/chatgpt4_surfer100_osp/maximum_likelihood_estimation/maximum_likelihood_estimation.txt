<INTRODUCTION>
Maximum Likelihood Estimation, or MLE, is a statistical method to determine parameters of a statistical model. Used in a wide range of statistical models across fields such as machine learning and economics, this technique provides an intuitive way to fit models to data by searching for parameter values that make the observed data most ‘likely’. The motivation behind MLE is to determine the underlying distribution and parameters that most probably resulted in the observed dataset.

<HISTORY>
R.A. Fisher, a British statistician and geneticist, first introduced the method of Maximum Likelihood Estimation in the early 20th century. He developed this method with the idea of pattern recognition in mind. Furthermore, he aimed to address problems related to determining the parameters of probability distributions, which could not be properly solved using previously available mathematical methods. MLE became fundamental in the field of statistics and has substantially influenced various scientific fields.

<KEY IDEAS>
The fundamental concept of Maximum Likelihood Estimation is to estimate the parameters of a model that maximize the likelihood function. The likelihood function is the joint distribution of observed data, with model parameters treated as variables. It measures the "compatibility" between a particular choice of parameters and the observed data. The choice that makes the observed data "most probable" is the one deemed the "best". This process involves setting the derivatives of the likelihood function with respect to the parameters to zero (the score equations) and solving them simultaneously to find the maximum likelihood estimates. 

<USES/APPLICATIONS>
In statistics and machine learning, the Maximum Likelihood Estimation method is widely applied for parameter estimation in numerous models like linear regression, logistic regression, and neural networks. Its usage spans to more complex tasks like speech recognition, natural language processing, and computer vision where models with multiple layers require optimization. MLE has been effectively used in econometrics and medical statistics for the prediction and analysis of data.

<VARIATIONS>
Several variants and extensions to the maximum likelihood estimation exist. One notable variation is Penalized Maximum Likelihood Estimation, which introduces a penalty term to the likelihood to prevent overfitting or to deal with multicollinearity. Bayesian inference can be seen as a generalization of MLE where we incorporate prior knowledge or beliefs about the parameters. Another variation is the Expectation-Maximization algorithm, which iteratively refines the parameter estimates when dealing with missing or hidden data. These variants demonstrate how the concept of likelihood can be adapted to suit different problems and contexts.